
* Grammar
** __global__  kernel_fun()
   - 告诉编译器把该函数放在GPU上运行
     
** blockIdx.x , threadIdx.x
   - 线程块数量不超过65535(参考)
   - 每个线程块线程数量不超过maxThreadsPerBlock(参考512)
   - gridDim : 二维的,线程格中每一维的线程块数量
   - blockDim : 三维的,线程块中每一维的线程数量

** kernel<<<N, 1>>>
   - 第一个参数表示设备在执行核函数时使用的并行线程块(Block)的数量
     #+BEGIN_EXAMPLE
     __global__ void add( int *a, int *b, int *c ) {
     int tid = blockIdx.x;    // this thread handles the data at its thread id
     if (tid < N)
        c[tid] = a[tid] + b[tid];
     }
     ...
     add<<<N,1>>>( dev_a, dev_b, dev_c );
     ...
     #+END_EXAMPLE
     #+BEGIN_EXAMPLE
     __global__ void kernel( unsigned char *ptr ) {
     // map from blockIdx to pixel position
     int x = blockIdx.x;
     int y = blockIdx.y;
     int offset = x + y * gridDim.x;
     }
     #+END_EXAMPLE
   - 第二个参数表示CUDA运行时在每个线程块中创建的线程数量

** __device__
   - 表示代码将在GPU而不是主机上运行,只能从其他__device__函数或者__global__函数中调用它们
     #+BEGIN_EXAMPLE
     struct cuComplex {
     float   r;
     float   i;
     cuComplex( float a, float b ) : r(a), i(b)  {}
     __device__ float magnitude2( void ) {
        return r * r + i * i;
     }
     __device__ cuComplex operator*(const cuComplex& a) {
        return cuComplex(r*a.r - i*a.i, i*a.r + r*a.i);
     }
     __device__ cuComplex operator+(const cuComplex& a) {
        return cuComplex(r+a.r, i+a.i);
     }
     };
     #+END_EXAMPLE

** __share__
   - 添加到变量声明中,使这个变量驻留在共享内存中
   - 每个线程块都拥有该共享内存的私有副本
     
** __syncthreads
   - 线程同步
     
** __constant__
   - 常量内存把变量的访问限制为只读
   - 常量内存,不再需要调用 cudaMalloc() 分配内存
   - cudaMemcpyToSymbol() 复制到常量内存
     
** cudaError_t cudaMalloc (void **devPtr, size_t  size )
   - 不能在主机代码中使用　cudaMalloc() 分配的指针进行内存读／写操作
   #+BEGIN_EXAMPLE
   #include "../common/book.h"
   __global__ void add( int a, int b, int *c ) {
    *c = a + b;
   }
   
   int main( void ) {
    int c;
    int *dev_c;
    HANDLE_ERROR( cudaMalloc( (void**)&dev_c, sizeof(int) ) );
    add<<<1,1>>>( 2, 7, dev_c );
    HANDLE_ERROR( cudaMemcpy( &c, dev_c, sizeof(int),
                              cudaMemcpyDeviceToHost ) );
    printf( "2 + 7 = %d\n", c );
    HANDLE_ERROR( cudaFree( dev_c ) );
    return 0;
   }

   #+END_EXAMPLE
* 常量内存与事件
* 纹理内存(Texture Memory)
** 只读内存
   - 与常量内存类似,纹理内存同样缓存在芯片上
   - 能够减少对内存的请求并提供更高效的内存带宽
   - 专门为那些在内存访问模式中存在大量空间局部性(Spatial Locality)的图形应用程序而设计的,
     在计算应用程序中,这意味着一个线程读取的位置可能与临近线程读取的位置非常接近.
* 流(Stream)
** 任务并行
   - 并行执行两个或多个不同的任务
** 页锁定主机内存
   - cudaMalloc()
     在GPU上分配内存
   - malloc()
     在主机上分配内存,可分页的(Pagable)主机内存
   - cudaHostAlloc()
     * 分配页锁定的主机内存(Pinned Memory),不可分页内存,操作系统将不会对这块内存分页并交换到磁盘上,
     从而保证了该内存始终驻留在物理内存中
     * 由于GPU知道内存的物理地址,可以通过 DMA(Direct Memory Access)技术来在GPU和主机之间复制数据.
   - cudaFreeHost()       
*** Usage
   - 对cudaMemcpy()调用中的源内存或目标内存,使用页锁定内存,在不再需要使用它们时立即释放,而不是等到
     应用程序关闭时才释放.

** Stream
*** CUDA流表示一个 GPU操作队列
    - 可以将每个流视作GPU上的一个任务
    - 流就像一个有序的工作队列,GPU从该队列中依次取出任务并执行
*** 设备重叠(Device Overlap)
    - 支持设备重叠功能的GPU能够在执行一个CUDA C核函数的同时,还能在设备与主机之间执行复制操作.
*** Usage
    - cudaStream_t stream
    - cudaStreamCreate(&stream)
    - cudaMemcpyAsync(dev_buf, cuda_host_buf, size, cudaMemcpyDeviceToHost, stream)
      在GPU和主机之间复制数据,只能以异步方式对页锁定内存进行复制操作
    - kernel<<< , , , stream>>>()
      核函数调用将是异步的
    - cudaStreamSynchronize(stream)
      主机继续执行之前,等待GPU执行完成
    - cudaStreamDestroy(stream)
      释放流
