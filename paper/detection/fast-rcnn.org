
* Fast R-CNN --Ross Girshick

Paper: [[http://arxiv.org/abs/1504.08083][Fast R-CNN]]
Code: [[https://github.com/rbgirshick/fast-rcnn][Fast R-CNN's code]]


** Fast R-CNN architecture and training 

   [[./pic_fast_rcnn/1.png]]
   - inputs: *an entire image*, *a set of object proposals*
   - several convolutional and max pooling layers -> produce a conv feature map
   - for each object proposal: a RoI(region of interest) pooling layer extracts a 
     fixed-length feature vector from the feature map
   - each feature vector is fed into a sequence of fully connected(fc) layers 
     that finally branch into two sibling(兄弟，姐妹，同属) output layers:
     *one* that produces softmax probability estimates over K object classes
     plus a catch-all "background" class and *another layer* that outputs 
     four real-valued numbers for each of the K object classes.
   - [ ] ? 2. /Each set of 4 values encodes refined bounding-box positions for one of
           the K classes./

*** The RoI pooling layer
    - The RoI pooling layer uses max pooling to convert the features inside any valid
    region of interest into a small feature map with a fixed spatial extent of HxW,
    wherer H and W are layer hyper-parameters that are independent of any particular RoI.

    - RoI: (r,c,h,w) specifies its top-left corner(r,c) and its height and width(h,w).

    - RoI max pooling layer divides the hxw RoI window into an HxW grid of sub-windows of
      approximate size h/H x w/W and then max-pooling the values in each sub-window into 
      the corresponding output grid cell.

*** Initializing from pre-trained networks

    - When a pre-trained network initializes a Fast R-CNN network, it undergoes three
      transformations:
      1. The last max pooling layer is replaced by a RoI pooling layer that is configured
         by setting H and W to be compatible with the net's first fully connected layer
         (e.g., H = W = 7 for VGG16).
      2. The network's last fully connected layer and softmax are replaced with the two 
         sibling layers described earlier: a fully connected layer and softmax over K + 1
         categories, category-specific bounding-box regressors.
      3. The network is modified to take two data inputs: a list of images and a list of
         RoIs in those images.

*** Fine-tuning for detection

    - In Fast R-CNN training, stochastic gradient descent(SGD) mini-batches are sampled 
      hierarchically.
      1. First sampling N images
      2. Second sampling R/N RoIs from each image.
    - RoIs from the same image share computation and memory in the forward and backward
      passes.
    - One concern over this strategy is it may cause slow training convergence because
      RoIs from the same image are correlated. This concern does not appear to be a 
      practical issue and we achieve good results with N = 2 and R = 128 using fewer
      SGD iterations than R-CNN.

**** Multi-task loss

     - A Fast R-CNN network has two sibling output layers.
       1. The first outputs a discrete probability distribution(per RoI), 
          $p = (p_0, ..., p_K)$, over K + 1 categories.(a softmax over the K + 1 outputs of a
          fully connected layer.
       2. The second sibling layer outputs bounding-box regression offsets, 
          $t^k = (t_x^k, t_y^k, t_w^k, t_h^k)$, for each of the K object classes, indexed by k.
       3. [ ] We use the parameterization for $t^k$ given in [fn:1], in which t^k specifies a
          scale-invariant translation and log-space height/width shift relative to an object 
          proposal.
     - *Each trainging RoI* is labeled with a ground-truth class u and a ground-truth bounding-box
       regression target v. We use a multi-task loss L on each labeled RoI to jointly train for
       classification and bounding-box regression:
       \begin{equation}
         L(p, u, t^u, v) = L_{cls}(p, u) + \lambda[u\ge1]L_{loc}(t^u, v)         
       \end{equation}
       in which $L_{cls}(p, u)  = -logp_u$ is log loss for true class u.
     - The second task loss , $L_loc$, is defined over a tuple of true bounding-box regression 
       targets for class u. The Iverson bracket indicator function $[u\ge1]$ evaluates to 1 when 
       $u>1$ and 0 otherwise.For background RoIs there is no notion of a ground-truth bounding box
       and hence $L_{loc}$ is ignored. For bounding-box regression, we use the loss
       \begin{equation}
         L_{loc}(t^u, v) = \sum_{i\in{x, y, w, h}} smooth_{L_1}(t_i^u - v_i)         
       \end{equation}
       in which 
       \begin{equation}
         smooth_{L_1}(x) = 
       \begin{cases}
       {0.5x^2} &\mbox{if |x| < 1}\\
       {|x| - 0.5} &\mbox{otherwise}
       \end{cases}
       \end{equation}
       is a robust $L_1$ loss that is less sensitive to outliers than the $L_2$ loss used in 
       R-CNN and SPPnet.
       - When the regression targets are unbounded, training with $L_2$ loss can require careful
         tuning of learning rates in order to prevent exploding gradients. Eq.3 eliminates this
         sensitivity.
     - We normalize the ground-truth regression targets $v_i$ to have zero mean and unit variance.
       All experiments use $\lambda = 1$.
     - [fn:2] uses a related loss to train a class agnostic object proposal network. [fn:2] advocates
       for a two-network system that separates localization and classification.
     - OverFeat[fn:3], R-CNN[fn:1], and SPPnet[fn:5] alse train classifiers and bounding-box 
       localizers, however these methods use stage-wise training, which we show is suboptimal
       for Fast R-CNN.

**** Mini-batch sampling

     1. During fine-tuning, each SGD mini-batch is constructed from N = 2 images, chosen uniformly
        at random. We use mini-batches of size R = 128, sampling 64 RoIs from each images.
     2. As in [fn:1], we take 25% of the RoIs from object proposals that have intersection over
        union(IoU) overlap with a ground-truth bounding box of at least 0.5. These RoIs comprise
        the examples labeled with a foreground object class, i.e. $u \ge 1$.
     3. The remaining RoIs are sampled from object proposals that have a maximum IoU with ground truth
        in the interval [0.1, 0.5), following [fn:5].
        1) These are the background examples and are labeled with u = 0.
        2) The lower threshold of 0.1 appears to act as a heuristic for hard example mining [fn:4].
     4. During traing, images are horizontally flipped with probability 0.5. No other data 
        augmentation is used.

**** Back-propagation through RoI pooling layers

     1. The RoI pooling layer's backwards function computes partial derivative of the loss
        function with respect to each input variable $x_i$ by following the argmax switches:
        \begin{equation}
          \frac{\partial{L}}{\partial{x_i}} = \sum_r\sum_j[i = i*(r,j)]\frac{\partial{L}}{\partial{y_{rj}}}
        \end{equation}
        - where $x_i\in{R}$ be the i-th activation input into the RoI pooling layer and 
        $y_{rj}$ be the layer's j-th output from the r-th RoI.
        - The RoI pooling layer computes $y_{rj}=x_{i*(r,j)}$, in which $i*(r,j)=argmax_{i^{'}\in{R(r,j)}}x_{i^{'}}$. 
        $R(r,j)$ is the index set of inputs in the sub-window over which the output unit $y_{rj}$ 
        max pools.

**** SGD hyper-parameters

     - The fully connected layers used for softmax classification and bounding-box regression
       are initialized from $N(0,0.01^2)$ and $N(0,0.001^2)$. Biases are initialized to 0.
     - All layers use a pre-layer learning rate of 1 for weights and 2 for biases and a global
       learning rate of 0.001.
     - When training on VOC07 or VOC12 trainval we run SGD for 30k mini-batch iterations, and
       then lower the learning rate to 0.0001 and train for another 10k iterations.
     - Momentum : 0.9 , Parameter decay : 0.0005(on weights and biases)

       


*** Scale invariance

    1. We explore two ways of achieving scale invariant object detection:
       1) via "brute force"
       2) by using image pyramids
    2. These strategies follow the two approaches in [fn:5].
    3. Brute-force approach
       - Each image is processed at a pre-defined pixel size during both training and testing.
       - The network must directly learn scale-invariant object detection from the training data.
    4. Multi-scale approach
       - Provides approximate scale-invariance to the network through an image pyramid.
       - At test-time, the image pyramid is used to approximately scale-normalize each object 
         proposal.
       - During multi-scale training, we randomly sample a pyramid scale each time an image is
         sampled, following [fn:5], as a form of data augmentation.
    5. We experiment with multi-scale training for smaller networks only, due to GPU memory limits.
          

* Footnotes

[fn:1] R. Girshick, J. Donahue, T. Darrell, and J. Malik.  
  Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.

[fn:2] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. 
Scalable object detection using deep neural networks. In CVPR, 2014.

[fn:3] P. Sermanet,  D. Eigen,  X. Zhang,  M. Mathieu,  R. Fergus,and Y. LeCun.  
OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks.  
In ICLR,2014.

[fn:4] P.  Felzenszwalb,  R.  Girshick,  D.  McAllester,  and  D.  Ramanan.   
Object detection with discriminatively trained part based models.
TPAMI, 2010.

[fn:5] K. He, X. Zhang, S. Ren, and J. Sun. 
Spatial pyramid pooling in  deep  convolutional  networks  for  visual  recognition.   
In ECCV, 2014. 





          
          
