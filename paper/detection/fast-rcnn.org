
* Fast R-CNN --Ross Girshick

Paper: [[http://arxiv.org/abs/1504.08083][Fast R-CNN]]
Code: [[https://github.com/rbgirshick/fast-rcnn][Fast R-CNN's code]]


** Fast R-CNN architecture

   [[./pic_fast_rcnn/1.png]]
   - inputs: *an entire image*, *a set of object proposals*
   - several convolutional and max pooling layers -> produce a conv feature map
   - for each object proposal: a RoI(region of interest) pooling layer extracts a 
     fixed-length feature vector from the feature map
   - each feature vector is fed into a sequence of fully connected(fc) layers 
     that finally branch into two sibling(兄弟，姐妹，同属) output layers:
     *one* that produces softmax probability estimates over K object classes
     plus a catch-all "background" class and *another layer* that outputs 
     four real-valued numbers for each of the K object classes.
   - [ ] ? 2. /Each set of 4 values encodes refined bounding-box positions for one of
           the K classes./

*** The RoI pooling layer
    - The RoI pooling layer uses max pooling to convert the features inside any valid
    region of interest into a small feature map with a fixed spatial extent of HxW,
    wherer H and W are layer hyper-parameters that are independent of any particular RoI.

    - RoI: (r,c,h,w) specifies its top-left corner(r,c) and its height and width(h,w).

    - RoI max pooling layer divides the hxw RoI window into an HxW grid of sub-windows of
      approximate size h/H x w/W and then max-pooling the values in each sub-window into 
      the corresponding output grid cell.

*** Initializing from pre-trained networks

    - When a pre-trained network initializes a Fast R-CNN network, it undergoes three
      transformations:
      1. The last max pooling layer is replaced by a RoI pooling layer that is configured
         by setting H and W to be compatible with the net's first fully connected layer
         (e.g., H = W = 7 for VGG16).
      2. The network's last fully connected layer and softmax are replaced with the two 
         sibling layers described earlier: a fully connected layer and softmax over K + 1
         categories, category-specific bounding-box regressors.
      3. The network is modified to take two data inputs: a list of images and a list of
         RoIs in those images.

*** Fine-tuning for detection

    - In Fast R-CNN training, stochastic gradient descent(SGD) mini-batches are sampled 
      hierarchically.
      1. First sampling N images
      2. Second sampling R/N RoIs from each image.
    - RoIs from the same image share computation and memory in the forward and backward
      passes.
    - One concern over this strategy is it may cause slow training convergence because
      RoIs from the same image are correlated. This concern does not appear to be a 
      practical issue and we achieve good results with N = 2 and R = 128 using fewer
      SGD iterations than R-CNN.

**** Multi-task loss

     - A Fast R-CNN network has two sibling output layers.
       1. The first outputs a discrete probability distribution(per RoI), 
          $p = (p_0, ..., p_K)$, over K + 1 categories.(a softmax over the K + 1 outputs of a
          fully connected layer.
       2. The second sibling layer outputs bounding-box regression offsets, 
          $t^k = (t_x^k, t_y^k, t_w^k, t_h^k)$, for each of the K object classes, indexed by k.
       3. [ ] We use the parameterization for $t^k$ given in [fn:1], in which t^k specifies a
          scale-invariant translation and log-space height/width shift relative to an object 
          proposal.
     - Each trainging RoI is labeled with a ground-truth class u and a ground-truth bounding-box
       regression target v. We use a multi-task loss L on each labeled RoI to jointly train for
       classification and bounding-box regression:
       $$L(p, u, t^u, v) = L_{cls}(p, u) + \lambda[u\ge1]L_{loc}(t^u, v)$$
       in which $L_{cls}(p, u)  = -logp_u$ is log loss for true class u.
     - The second task loss , $L_loc$, is defined over a tuple of true bounding-box regression 
       targets for class u. The Iverson bracket indicator function $[u\ge1]$ evaluates to 1 when 
       $u>1$ and 0 otherwise.For background RoIs there is no notion of a ground-truth bounding box
       and hence $L_{loc}$ is ignored. For bounding-box regression, we use the loss
       $$L_{loc}(t^u, v) = \sum_{i\in{x, y, w, h}} smooth_{L_1}(t_i^u - v_i)$$
       in which 
       $$smooth_{L_1}(x) = 
       

* Footnotes

[fn:1] R. Girshick, J. Donahue, T. Darrell, and J. Malik.  
  Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.






          
          
